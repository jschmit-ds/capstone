{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Base Location Data For Each Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas  as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h3 as h3\n",
    "\n",
    "# import libraries needed for upload / download to AWS\n",
    "import boto3\n",
    "import awswrangler\n",
    "from fiona.session import AWSSession\n",
    "import fiona\n",
    "# set name of S3 bucket\n",
    "s3_bucket = 'traffic-data-bucket'\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "path =  Path(os.getcwd())\n",
    "root = path.parent.absolute()\n",
    "\n",
    "root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Base Table with node/edge info and city/district lables\n",
    "##### LA County shape file transposed to Uber Hexegons at level 10. ~150 square meters\n",
    "##### This process takes a shape file and maps it to hex files for a given level. The output of the mapping is the a unique hex_id for the hexegon and the shape geometry\n",
    "##### https://h3geo.org/docs/core-library/restable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aws_secrets import aws_access_key_id, aws_secret_access_key, aws_session_token\n",
    "\n",
    "my_session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token = aws_session_token\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fiona.Env(session=AWSSession(my_session)):\n",
    "    gdf_all = gpd.read_file(f's3://{s3_bucket}/h3_processed_data/base_map_hex_all/base_map_hex_all.shp')\n",
    "\n",
    "\n",
    "print(gdf_all.shape)\n",
    "gdf_all.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rking_neighbors(row, skins):\n",
    "    neighbors = h3.k_ring(row.hex_id, skins)\n",
    "    neighbors_list = list(neighbors)\n",
    "    return(neighbors_list)\n",
    "\n",
    "gdf_all['hex_neighbors_1_ids'] = gdf_all.apply(lambda x: rking_neighbors(x, skins = 1), axis=1)\n",
    "gdf_all['hex_neighbors_2_ids'] = gdf_all.apply(lambda x: rking_neighbors(x, skins = 2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Collision Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collisions = pd.read_csv(root / 'X.data' / 'h3_processed_data' / 'collisions_hex.csv')\n",
    "collisions = awswrangler.s3.read_csv(path=f's3://{s3_bucket}/h3_processed_data/collisions_hex.csv', boto3_session=my_session)\n",
    "\n",
    "collisions = collisions[~(collisions.hex_id == '0')]\n",
    "print(collisions.shape)\n",
    "collisions.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Collisions \n",
    "\n",
    "### 2.1 Prep collision file by making a wide table.  One unique row per hex id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions_year_grp = collisions.groupby([\"hex_id\", \"collision_year\"])[\"accident_count\"].agg('sum').reset_index()\n",
    "print(collisions_year_grp.columns)\n",
    "years = list(collisions_year_grp.collision_year.unique())\n",
    "years.sort()\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions_year_grp['hex_neighbors_1_ids'] = collisions_year_grp.apply(lambda x: rking_neighbors(x, skins = 1), axis=1)\n",
    "collisions_year_grp['hex_neighbors_2_ids'] = collisions_year_grp.apply(lambda x: rking_neighbors(x, skins = 2), axis=1)\n",
    "collisions_year_grp.sample(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions_year_grp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions_year_grp[['hex_id', 'hex_neighbors_1_ids']].explode('hex_neighbors_1_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for year in years[1:]:\n",
    "coll_year_dict = {}\n",
    "gdf_all_join = gdf_all[[\"hex_id\", \"hex_neighbors_1_ids\", \"hex_neighbors_2_ids\"]]\n",
    "\n",
    "for year in years+[2022]:\n",
    "#for year in [2020]:\n",
    "    print(year)\n",
    "    gdf_all_join = gdf_all[[\"hex_id\", \"hex_neighbors_1_ids\", \"hex_neighbors_2_ids\"]]\n",
    "    df_prev1 = collisions_year_grp[[\"hex_id\", \"accident_count\"]][collisions_year_grp.collision_year == (year-1)]\n",
    "    #print(df_prev1.columns)\n",
    "    df_prev2 = collisions_year_grp[[\"hex_id\", \"accident_count\"]][collisions_year_grp.collision_year == (year-2)]\n",
    "    gdf_all_join = gdf_all_join.merge(df_prev1, on = 'hex_id', how = 'left')\n",
    "    gdf_all_join = gdf_all_join.merge(df_prev2, on = 'hex_id', how = 'left').fillna(0)\n",
    "    #display(gdf_all_join.sample())\n",
    "    gdf_all_join.columns = [\"hex_id\", \"hex_neighbors_1_ids\", \"hex_neighbors_2_ids\", \"prev1_yr_coll_cnt\", \"prev2_yr_coll_cnt\"]\n",
    "    \n",
    "    #1 yr prev\n",
    "    #ring 1\n",
    "    \n",
    "    gdf_all_join_tall = gdf_all_join[['hex_id', 'hex_neighbors_1_ids']].explode('hex_neighbors_1_ids')\n",
    "    gdf_all_join_tall = gdf_all_join_tall.merge(df_prev1[['hex_id', 'accident_count']], \n",
    "                                        left_on = 'hex_neighbors_1_ids', \n",
    "                                        right_on = 'hex_id',\n",
    "                                        how = 'inner')\n",
    "    n_grp = gdf_all_join_tall.groupby('hex_id_x')['accident_count'].agg('sum').reset_index()\n",
    "    n_grp.columns = ['hex_id_x', 'prev1_yr_coll_neighbor1']\n",
    "    n_grp['prev1_yr_coll_neighbor1_ave'] = n_grp['prev1_yr_coll_neighbor1']/7\n",
    "    gdf_all_join = gdf_all_join.merge(n_grp, left_on = 'hex_id', right_on = 'hex_id_x', how = 'left')\n",
    "    gdf_all_join.drop(columns='hex_id_x', inplace = True)\n",
    "    #ring 2\n",
    "    gdf_all_join_tall = gdf_all_join[['hex_id', 'hex_neighbors_2_ids']].explode('hex_neighbors_2_ids')\n",
    "    gdf_all_join_tall = gdf_all_join_tall.merge(df_prev1[['hex_id', 'accident_count']], \n",
    "                                        left_on = 'hex_neighbors_2_ids', \n",
    "                                        right_on = 'hex_id',\n",
    "                                        how = 'inner')\n",
    "    n_grp = gdf_all_join_tall.groupby('hex_id_x')['accident_count'].agg('sum').reset_index()\n",
    "    n_grp.columns = ['hex_id_x', 'prev1_yr_coll_neighbor2']\n",
    "    n_grp['prev1_yr_coll_neighbor2_ave'] = n_grp['prev1_yr_coll_neighbor2']/19\n",
    "    gdf_all_join = gdf_all_join.merge(n_grp, left_on = 'hex_id', right_on = 'hex_id_x', how = 'left')\n",
    "    gdf_all_join.drop(columns='hex_id_x', inplace = True)\n",
    "    \n",
    "    #2 yr prev\n",
    "    #ring 1\n",
    "    gdf_all_join_tall = gdf_all_join[['hex_id', 'hex_neighbors_1_ids']].explode('hex_neighbors_1_ids')\n",
    "    gdf_all_join_tall = gdf_all_join_tall.merge(df_prev2[['hex_id', 'accident_count']], \n",
    "                                        left_on = 'hex_neighbors_1_ids', \n",
    "                                        right_on = 'hex_id',\n",
    "                                        how = 'inner')\n",
    "    n_grp = gdf_all_join_tall.groupby('hex_id_x')['accident_count'].agg('sum').reset_index()\n",
    "    n_grp.columns = ['hex_id_x', 'prev2_yr_coll_neighbor1']\n",
    "    n_grp['prev2_yr_coll_neighbor1_ave'] = n_grp['prev2_yr_coll_neighbor1']/7\n",
    "    gdf_all_join = gdf_all_join.merge(n_grp, left_on = 'hex_id', right_on = 'hex_id_x', how = 'left')\n",
    "    gdf_all_join.drop(columns='hex_id_x', inplace = True)\n",
    "    #2 yr prev\n",
    "    gdf_all_join_tall = gdf_all_join[['hex_id', 'hex_neighbors_2_ids']].explode('hex_neighbors_2_ids')\n",
    "    gdf_all_join_tall = gdf_all_join_tall.merge(df_prev2[['hex_id', 'accident_count']], \n",
    "                                        left_on = 'hex_neighbors_2_ids', \n",
    "                                        right_on = 'hex_id',\n",
    "                                        how = 'inner')\n",
    "    n_grp = gdf_all_join_tall.groupby('hex_id_x')['accident_count'].agg('sum').reset_index()\n",
    "    n_grp.columns = ['hex_id_x', 'prev2_yr_coll_neighbor2']\n",
    "    n_grp['prev2_yr_coll_neighbor2_ave'] = n_grp['prev2_yr_coll_neighbor2']/19\n",
    "    gdf_all_join = gdf_all_join.merge(n_grp, left_on = 'hex_id', right_on = 'hex_id_x', how = 'left')\n",
    "    gdf_all_join.drop(columns='hex_id_x', inplace = True)\n",
    "    \n",
    "    #print(gd_all_1_tall.columns)\n",
    "    coll_year_dict[year] = gdf_all_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coll_year_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_year_dict[2022]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload to S3 bucket as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of folder that will store the data uploaded to s3\n",
    "raw_path_dir = 'joined_data'\n",
    "\n",
    "for key in coll_year_dict:\n",
    "    df = coll_year_dict[key]\n",
    "    print(key)\n",
    "    raw_path = f\"s3://{s3_bucket}/{raw_path_dir}/{'base_location_' + str(key) + '_collision_data.csv'}\"\n",
    "    awswrangler.s3.to_csv(df=df, path = raw_path, index=False,\n",
    "                       boto3_session=my_session, use_threads=True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
