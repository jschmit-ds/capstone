{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas  as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h3 as h3\n",
    "\n",
    "import re\n",
    "\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "path =  Path(os.getcwd())\n",
    "root = path.parent.absolute()\n",
    "\n",
    "root\n",
    "\n",
    "# import aws libraries\n",
    "import boto3\n",
    "import awswrangler\n",
    "# set name of S3 bucket\n",
    "s3_bucket = 'traffic-data-bucket'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.1 Import Base Table with Valid Street ID for sampling\n",
    "##### LA County shape file transposed to Uber Hexegons at level 8. ~.75 square km\n",
    "##### This process takes a shape file and maps it to hex files for a given level. The output of the mapping is the a unique hex_id for the hexegon and the shape geometry\n",
    "##### https://h3geo.org/docs/core-library/restable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aws_secrets import aws_access_key_id, aws_secret_access_key, aws_session_token\n",
    "\n",
    "my_session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token = aws_session_token\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_all = awswrangler.s3.read_csv(path=f's3://{s3_bucket}/joined_data/base_location_data.csv', boto3_session=my_session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_mask = gdf_all['valid_accident_location_filter'] == True\n",
    "gdf_valid = gdf_all[valid_mask]\n",
    "gdf_valid = gdf_valid[['hex_id']]\n",
    "gdf_valid.shape\n",
    "display(gdf_valid.sample(3))\n",
    "del(gdf_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Collision Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions = awswrangler.s3.read_csv(path=f's3://{s3_bucket}/h3_processed_data/collisions_hex.csv', boto3_session=my_session)\n",
    "\n",
    "# collisions = pd.read_csv(root / 'X.data' / 'h3_processed_data' / 'collisions_hex.csv')\n",
    "collisions = collisions[~(collisions.hex_id == '0')]\n",
    "print(collisions.shape)\n",
    "collisions.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Collisions \n",
    "\n",
    "### 2.1 Prep collision file by making a wide table.  One unique row per hex id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collisions_year_counts = collisions.groupby([\"collision_year\"])[\"count\"].agg('sum').reset_index()\n",
    "collision_year_count = collisions.groupby(\"collision_year\").hex_id.agg(\"count\").to_frame(\"count\").reset_index()\n",
    "display(collision_year_count)\n",
    "collisions = collisions[[\"hex_id\", \"collision_year\", \"collision_dayofyear\"]]\n",
    "collisions = collisions.drop_duplicates()\n",
    "print(collisions.columns)\n",
    "coll_years = list(collisions.collision_year.unique())\n",
    "print(coll_years.sort())\n",
    "collisions.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision_year_count[collision_year_count.collision_year == 2018]['count'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rking_neighbors(row, skins):\n",
    "    neighbors = h3.k_ring(row.hex_id, skins)\n",
    "    neighbors_list = list(neighbors)\n",
    "    return(neighbors_list)\n",
    "\n",
    "collisions['hex_neighbors_2_ids'] = collisions.apply(lambda x: rking_neighbors(x, skins = 2), axis=1)\n",
    "collisions.sample(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "def get_fence_range(doy):\n",
    "    #daysinyear = 365\n",
    "    #is_leapyear = calendar.isleap(year)\n",
    "    #if is_leapyear:\n",
    "    #    daysinyear = 366\n",
    "    doy_fence = \"\"\n",
    "    for i in range((doy-3),(doy+3)):\n",
    "        #print(i)\n",
    "        doy_fence = doy_fence + \" \" +str(i)\n",
    "    doy_fence = doy_fence.strip()\n",
    "    return(doy_fence)\n",
    "\n",
    "def collision_fence(row):\n",
    "    doy = int(row.doy_fence)\n",
    "    year = int(row.collision_year)\n",
    "    \n",
    "    doy_out = doy\n",
    "    year_out = year\n",
    "    \n",
    "    daysinyear = 365\n",
    "    daysinyear_prev = 365 \n",
    "    \n",
    "    is_leapyear = calendar.isleap(year)\n",
    "    if is_leapyear:\n",
    "        daysinyear = 366\n",
    "    is_leapyear = calendar.isleap(year - 1)\n",
    "    if is_leapyear:\n",
    "        daysinyear_prev = 366\n",
    "\n",
    "    #days after year end\n",
    "    if daysinyear < doy:\n",
    "        year_out = year + 1\n",
    "        doy_out = doy - daysinyear \n",
    "    #days before year began\n",
    "    if doy < 0:\n",
    "        year_out = year -1\n",
    "        doy_out = daysinyear_prev + doy\n",
    "    return pd.Series([year_out, doy_out])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the collision day of year create a list with plus minus 3 days\n",
    "#so we will not be taking samples from any of the dqys just before a collision and just after\n",
    "collisions['doy_fence'] = collisions['collision_dayofyear'].apply(lambda x: get_fence_range(x))\n",
    "collisions['doy_fence'] = collisions['doy_fence'].str.split()\n",
    "collisions['doy_fence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explode the column to create a row for every day of year in the list\n",
    "collisions = collisions[['hex_id', 'collision_year', 'hex_neighbors_2_ids', 'doy_fence']].explode('doy_fence')\n",
    "collisions.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collision fence take collision year and day fence and adjusts for days that run into another year. For example, -1 2010 would be 365 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example doy -1 2010, would become day 365 2009\n",
    "collisions[['year_fence', 'doy_fence']] = collisions.apply(collision_fence ,axis=1)\n",
    "collisions.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision_dict = {}\n",
    "for year in coll_years:\n",
    "    year_mask = collisions['year_fence'] == year\n",
    "    collision_dict[year] = collisions[year_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del collisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate negative samples\n",
    "Here we will generate 4 negative samples for each collision.\n",
    "\n",
    "Start by designating a folder to store the negative sample data in the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path_dir = 'neg_samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_multiplier = 4\n",
    "#sample_year = 2018\n",
    "\n",
    "neg_sample_dict = {}\n",
    "\n",
    "for sample_year in coll_years:\n",
    "    print(sample_year)\n",
    "    \n",
    "    #assign days in a year\n",
    "    daysinyear = 365\n",
    "    #correct for leap years\n",
    "    is_leapyear = calendar.isleap(sample_year)\n",
    "    if is_leapyear:\n",
    "        daysinyear = 366\n",
    "    \n",
    "    doy = \"\"\n",
    "    #python day of year starts at 1 not zero\n",
    "    for i in range(1,daysinyear+1):\n",
    "        doy = doy + \" \" +str(i)\n",
    "    doy = doy.strip()\n",
    "    \n",
    "    #attach a vector of all the days of yeat to each hex id\n",
    "    gdf_valid['doy'] = doy\n",
    "    gdf_valid['doy'] = gdf_valid['doy'].str.split()\n",
    "    print(gdf_valid.shape)\n",
    "    display(gdf_valid.head(1))\n",
    "    #make a tall table, one column for every hex id and day of year\n",
    "    gdf_valid_exp = gdf_valid[['hex_id', 'doy']].explode('doy')\n",
    "\n",
    "    #attache a column for doy\n",
    "    gdf_valid_exp['year'] = sample_year\n",
    "    gdf_valid_exp.shape\n",
    "    \n",
    "    \n",
    "    #get the frame from the dictionary for the given year\n",
    "    #create samples by making a refence of all accidents on a given day within 2 neighbors of a hexegon.\n",
    "    \n",
    "    coll_ref = collision_dict[sample_year]\n",
    "    #name the columns\n",
    "    coll_ref = coll_ref[['hex_neighbors_2_ids', 'doy_fence', 'year_fence']]\n",
    "    sample_year_initial_count = len(collision_dict[sample_year]['hex_id'].unique())\n",
    "    print('exploding collisions')\n",
    "    coll_ref = coll_ref.explode('hex_neighbors_2_ids')\n",
    "    coll_ref = coll_ref.drop_duplicates()\n",
    "    \n",
    "    coll_ref['doy_fence'] = coll_ref['doy_fence'].map(str)\n",
    "    gdf_valid_exp['doy'] = gdf_valid_exp['doy'].map(str)\n",
    "    print('merging data')\n",
    "    gdf_valid_exp = gdf_valid_exp.merge(coll_ref, \n",
    "                                left_on = ['hex_id', 'doy'],\n",
    "                                right_on = ['hex_neighbors_2_ids', 'doy_fence'],\n",
    "                                how = 'left')\n",
    "    \n",
    "    #remove any samples that are in accident fence\n",
    "    #after joining, accidents to exclude will have a valid neighbor id.  All nas are therefore eligible for sampling\n",
    "    nas_mask = gdf_valid_exp.hex_neighbors_2_ids.isna()\n",
    "    #exclude is not na\n",
    "    gdf_valid_exp = gdf_valid_exp[~nas_mask]\n",
    "    \n",
    "    sample_year_initial_count = collision_year_count[collision_year_count.collision_year == sample_year]['count'].values[0]\n",
    "    negative_year_samples = gdf_valid_exp.sample(n = (sample_year_initial_count * sample_multiplier), replace = True, random_state = 42)\n",
    "    \n",
    "    negative_year_samples = negative_year_samples[['hex_id','doy','year']]\n",
    "    print(negative_year_samples.shape)\n",
    "    # negative_year_samples.to_csv(root / 'X.data' / 'neg_samples' / ('neg_samples_' + str(sample_year) + '.csv'), index = False )\n",
    "    \n",
    "    # create S3 file path for dataframe and upload to S3 bucket\n",
    "    raw_path = f\"s3://{s3_bucket}/{raw_path_dir}/{'neg_samples_' + str(sample_year) + '.csv'}\"\n",
    "    awswrangler.s3.to_csv(df=negative_year_samples, path = raw_path, index=False,\n",
    "                       boto3_session=my_session, use_threads=True\n",
    "                       )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone]",
   "language": "python",
   "name": "conda-env-capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
